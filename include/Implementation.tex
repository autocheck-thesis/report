\chapter{Implementation}

Brief introduction...

\section{Phoenix Framework}

Because of the choice of using Elixir, we had the opportunity to use the widely popular web framework Phoenix~\cite{Phoenix}. It is the \textit{de facto} standard Elixir library for developing websites. It is built upon simple constructs and does not have many moving parts. It is extensible using plugs -- a kind of per-connection middleware. Plugs are configured to be run on specific routes and they are used to handle everything from header verification and authentication to file uploads.

\subsection{Verifying LTI launch requests}

We must verify LTI launch requests as described in section \ref{ensuring_integrity}. To achieve this we found the open source plug 'plug\_lti'. The library had the required functionality we wanted but was erroneously implemented by presuming that the sent parameters were encoded as 'x-www-form-urlencoded', causing any space character to be encoded as a plus symbol. This caused the OAuth signature to be invalid, making every attempt at launching our LTI application fail. We fixed this by forking the library and replacing the encoding function, and using said fork in our Phoenix project.

As the library is implemented as a plug, it can be used in any context where plugs are supported in Phoenix -- both in router scopes and directly in controllers. We use it directly in our index controller, which acts as our LTI entrypoint (see plug directive in figure \ref{pluglti_listing}). The launch action in the controller is then responsible to redirect the user to the appropriate route according to launch parameters that we now can trust because of the verification, such as the user role or the assignment id.

\section{Assignment creation}

\label{pluglti_listing}
\begin{listing}
\begin{minted}{elixir}
defmodule ThesisWeb.IndexController do
  use ThesisWeb, :controller

  plug PlugLti when action in [:launch]
  
  def launch(
        conn,
        %{
          "user_id" => lti_user_id,
          "roles" => roles,
          "ext_lti_assignment_id" => assignment_id,
          "resource_link_title" => assignment_name
        } = _params
      ) do
    with {:ok, user} <-
            Thesis.Repo.get_or_insert(
              Thesis.User,
              %{lti_user_id: lti_user_id}
            ),
          {:ok, assignment} <-
            Thesis.Repo.get_or_insert(
              Thesis.Assignment,
              %{id: assignment_id, name: assignment_name}
            ) do
      role = determine_role(roles)
  
      conn
      |> put_session(:user, user)
      |> put_session(:role, role)
      |> redirect_user(role, assignment)
    else
      error -> raise error
    end
  end
  
  # ...
  
end
\end{minted}
\caption{Index controller having its launch action protected by the LTI verification plug.}
\end{listing}

% todo should we talk about how we updated the plug? Is it even interesting?

\section{Student submissions}

We must be able to accept student submissions, and make sure that a code runner is able to run tests on said submissions.

\subsection{Handling file uploads}

To handle file uploads we thought of a few different approaches.

We can either handle uploads ourselves or have the upload form talk either directly or indirectly with some blob storage service such as S3, Google Cloud Storage or Azure Storage. If we handle it indirectly, we let the file data bounce on our server, but the resulting storage location would be the same.

Handling the uploads ourselves is easy, as we can rely on Plug.Upload, which stores any uploaded file in a temporary directory and appends a struct with file details to the post parameters in a controller action. The biggest problem with handling the uploads ourselves is that anyone running the system would be required to monitor the storage usage and know how to increase the storage capacity.

The files can either be stored directly on disk, or indirectly in the database which can be running on a different host entirely, having its own storage.

Because of the scope of the project, we decide to store the submissions on disk, on our server. In a production environment, we would recommend using a blob storage service such as the ones mentioned above.

\subsubsection{Access control}

% todo better wording
Having the files publicly available to the Internet would not be good. This requires some kind of access control. We thought of a few different ways of handling this.

One way is to restrict access to specific hosts on a network level. If we restrict the access to only let the code runner Docker instance tasked to run the tests specific to a submission download we would have to trust the provided submission code. The submission code could theoretically download any other submission files on our server.

The other way is to restrict access to the submission files by requiring an authentication token, generated upon starting a code runner. This makes sure that a code runner, tasked with running tests on a specific submission, can only download that submission.

If a blob storage service is used, it can most likely be configured to only allow access to clients providing the authentication token.

\section{DSL}
One of the stated goals of this project was to lower the barrier of entry for teachers who want to set up automatic assignment testing for their courses. One key aspect for/to/when achieving this is test configuration. In order to decrease the amount of boilerplate written and facilitate a robust testing flow, we decided to design our own DSL. The intention was to create a language tailored specifically for this task, and implemented in Elixir, as described in section \ref{dsl_elixir}.

Listing \ref{dsl_listing} showcases an example configuration defined with our DSL. At the top.....

\begin{listing}
\begin{minted}{elixir}
@environment "elixir",
	version: "1.2.2",
	otp_release: "18.2.1"

@require_files "hello.exs", "dependency.js"

step "check grammar" do
	grammar_check
end

step "exit code test" do
	command "exit 1", ignore_exit_code: true
end

step "format all code" do
	#cmd "mix formatter ./**/*.{ex,exs}"
	format_code
end

test "check status code" do
	cmd "mix -S #{}
\end{minted}
\caption{Example DSL configuration}
\label{dsl_listing}
\end{listing}

\section{Deployment}

When deploying our project for production, we must change couple of things. Even though there are deployment infrastructure such as Heroku~\cite{CloudHeroku} or Dokku~\cite{DokkuSeen}, which automatically figures out the build environment, we are required to set up three things in the build process:

\begin{enumerate}
\item Handle of the application secrets through environment variables
\item Compile the application assets
\item Configure how to start the server in production
\end{enumerate}

\subsection{Application secrets}

The project has application secrets such as username and password to the database, secret key base used in cryptographic operations and certificates used in communication with the Docker daemon. These are hard-coded values when in an development environment, but have to be handled with care in production.

You are able to hard-code secrets in production too, but it comes with risks such as a developer publishing the configuration accidentally to Git. Therefore, injecting them via environment variables is the preferred way. How this is done can be seen in listing~\ref{configure_env_variables}.

\begin{listing}
\inputminted{bash}{code/env.sh}
\caption{Exporting environment variables used by the production application}
\label{configure_env_variables}
\end{listing}

We are also able to build deployment artifacts using Distillery~\cite{HomeDocumentation}.

